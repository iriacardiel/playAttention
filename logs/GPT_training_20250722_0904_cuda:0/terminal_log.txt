Running train_gpt.py...
torchrun --standalone --nproc_per_node=1 train_gpt.py
DDP mode
DDP active = True. Device: cuda:0 of type cuda: ddp_rank = 0, ddp_local_rank = 0, ddp_world_size = 1
HYPERPARAMETERS
{
  "compute_device": "cuda:0",
  "selected_tokenizer": "CharTokenizer",
  "vocab_size": null,
  "seq_size": 8,
  "batch_size": 32,
  "n_embd": 32,
  "n_head": 4,
  "flash_attention": false,
  "n_layer": 3,
  "dropout": 0.0,
  "tokens_per_step": 256,
  "training_steps": 20000,
  "lr": 0.001,
  "lr_warmup_steps": 0,
  "lr_decay": false,
  "gradient_clipping": false,
  "beta1": 0.9,
  "beta2": 0.999,
  "eps": 1e-8,
  "weight_decay": 0.0,
  "eval_loss_steps": 100,
  "eval_interval": 100,
  "train_val_ratio": 0.9
}
DATA PREPARATION
loaded 1115394 tokens
1 epoch = 4357 batches

Tokenziation summary:
  Tokenizer: CharTokenizer
  Tokenized text: 1,115,394 tokens
  Vocabulary size: 65 unique tokens

Data split:
  Training:   1,003,854 tokens (90.0%)
  Validation: 111,540 tokens (10.0%)

loaded 1115394 tokens
1 epoch = 4357 batches

Tokenziation summary:
  Tokenizer: CharTokenizer
  Tokenized text: 1,115,394 tokens
  Vocabulary size: 65 unique tokens

Data split:
  Training:   1,003,854 tokens (90.0%)
  Validation: 111,540 tokens (10.0%)

loaded 1115394 tokens
1 epoch = 4357 batches

Tokenziation summary:
  Tokenizer: CharTokenizer
  Tokenized text: 1,115,394 tokens
  Vocabulary size: 65 unique tokens

Data split:
  Training:   1,003,854 tokens (90.0%)
  Validation: 111,540 tokens (10.0%)


BATCH CALCULATIONS
Macro batch size: 256 tokens
Total batch size (B*T): 32 * 8 = 256 tokens
Grad accumulation steps (tokens_per_step // (total_batch_size*ddp_world_size)): 1

MODEL INITIALIZATION

Model Details:
  Total parameters: 42,304
  Trainable parameters: 42,304
  Model size: ~0.16 MB (float32)

TRAINING
Training steps: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [08:41<00:00, 38.38step/s, train_loss_acc 1.656728744506836]

Training completed in 0:08:41.171989 (HH:MM:SS)
Generated text: <START> 
Hemine
For word
To dave, as what do commost pach of bid these! O, shall feman, shall and thy the
Is all floas,
That
I'e know I'll matte.

CORIOLANUS:
If he a me stay froms mmell from you on shall of his to beet.

VOLUMNIA:
May wide holt:
Hever not the son daward,-.

VOLUMNENIUS:
Let us peopes a most them, beape ought previppet.

COMINIUS:
Veope were I many hour if'd fress:
You have to in feadet
Again been Citizens olque.

KINIUS:
I' amerf.

MENENIUS:
Let if make you shall sweet hope;
And shall y <END>