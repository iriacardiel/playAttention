{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f3cf7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f2661b",
   "metadata": {},
   "source": [
    "##### Dense - Fully Connected (FC) Layer - nn.Linear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59b5200",
   "metadata": {},
   "source": [
    "```nn.Linear(in_features, out_features)```\n",
    "\n",
    "is equivalent to \n",
    "\n",
    "$ y = xW^T+b $ shape [1]\n",
    "\n",
    "- $ x = [3,4,5] $ shape [1x3]\n",
    "\n",
    "- $ w = [w_1, w_2, w_3] $ shape [1x3]\n",
    "\n",
    "- $ b = b $ shape [1]\n",
    "\n",
    "```in_features = 3```\n",
    "```out_features = 1```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42fdd8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = Parameter containing:\n",
      "tensor([[-0.4424,  0.4990, -0.0855]], requires_grad=True)\n",
      "\n",
      "b = Parameter containing:\n",
      "tensor([-0.3704], requires_grad=True)\n",
      "\n",
      "y = tensor([-0.0713], grad_fn=<ViewBackward0>)\n",
      "\n",
      "y_manual = tensor([-0.0713], grad_fn=<AddBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Intantiate the layer\n",
    "layer = nn.Linear(in_features=3, out_features=1, bias=True)  # weight shape = [1×3], bias = [1]\n",
    "print(f\"w = {layer.weight}\\n\")  # [1 × 3] torch tensor randomly initialized\n",
    "print(f\"b = {layer.bias}\\n\")    # [1] torch tensor randomly initialized\n",
    "\n",
    "# Use the layer and check result\n",
    "x = torch.tensor([1.0, 2.0, 3.0]) # input shape = [3]\n",
    "y = layer(x) # output shape = [1]\n",
    "#y_manual = x @ layer.weight.T + layer.bias # output shape = [1]\n",
    "y_manual = torch.matmul(x, layer.weight.T) + layer.bias # output shape = [1]\n",
    "\n",
    "assert y == y_manual\n",
    "print(f\"y = {y}\\n\")\n",
    "print(f\"y_manual = {y_manual}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1797f0ba",
   "metadata": {},
   "source": [
    "##### Self-Attention: Minimal Chronological Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e124a8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights:\n",
      " [[0.4223188  0.1553624  0.4223188 ]\n",
      " [0.01587624 0.86681333 0.11731043]\n",
      " [0.1553624  0.4223188  0.4223188 ]]\n",
      "Self-Attention Output:\n",
      " [[0.8446376  0.73304361 0.8446376  0.73304361]\n",
      " [0.13318667 1.85093709 0.13318667 1.85093709]\n",
      " [0.5776812  1.26695639 0.5776812  1.26695639]]\n"
     ]
    }
   ],
   "source": [
    "# 1. Input: 3 tokens, each with 4 features\n",
    "X = np.array([\n",
    "    [1, 0, 1, 0],  # Token 1\n",
    "    [0, 2, 0, 2],  # Token 2\n",
    "    [1, 1, 1, 1],  # Token 3\n",
    "])  # Shape (seq_len=3, d_model=4)\n",
    "\n",
    "# 2. For reproducibility: manually initialize to identity\n",
    "W_q = np.eye(4)  # Identity for simplicity\n",
    "W_k = np.eye(4)\n",
    "W_v = np.eye(4)\n",
    "\n",
    "# 3. Compute Q, K, V\n",
    "Q = X @ W_q  # Shape (3, 4)\n",
    "K = X @ W_k  # Shape (3, 4)\n",
    "V = X @ W_v  # Shape (3, 4)\n",
    "\n",
    "# 4. Compute attention scores (Q @ K^T)\n",
    "scores = Q @ K.T  # Shape (3, 3)\n",
    "\n",
    "# 5. Scale scores\n",
    "d_k = Q.shape[1]\n",
    "scaled_scores = scores / np.sqrt(d_k)\n",
    "\n",
    "# 6. Softmax to get attention weights\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # stable softmax\n",
    "    return e_x / np.sum(e_x, axis=1, keepdims=True)\n",
    "\n",
    "attention_weights = softmax(scaled_scores)  # Shape (3, 3)\n",
    "\n",
    "# 7. Compute weighted sum of V\n",
    "output = attention_weights @ V  # Shape (3, 4)\n",
    "\n",
    "# 8. Done — print results\n",
    "print(\"Attention Weights:\\n\", attention_weights)\n",
    "print(\"Self-Attention Output:\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35e8192",
   "metadata": {},
   "source": [
    "##### Self-Attention with torch.nn.Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b868e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights:\n",
      " tensor([[0.4223, 0.1554, 0.4223],\n",
      "        [0.0159, 0.8668, 0.1173],\n",
      "        [0.1554, 0.4223, 0.4223]], grad_fn=<SoftmaxBackward0>)\n",
      "Self-Attention Output:\n",
      " tensor([[0.8446, 0.7330, 0.8446, 0.7330],\n",
      "        [0.1332, 1.8509, 0.1332, 1.8509],\n",
      "        [0.5777, 1.2670, 0.5777, 1.2670]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 1. Input: 3 tokens, each with 4 features\n",
    "X = torch.tensor([\n",
    "    [1.0, 0.0, 1.0, 0.0],  # Token 1\n",
    "    [0.0, 2.0, 0.0, 2.0],  # Token 2\n",
    "    [1.0, 1.0, 1.0, 1.0],  # Token 3\n",
    "])  # Shape: (seq_len=3, d_model=4)\n",
    "\n",
    "# 2. Linear layers for Q, K, V. For reproducibility: manually override and initialize to identity\n",
    "d_model = 4\n",
    "linear_q = nn.Linear(d_model, d_model, bias=False)\n",
    "linear_k = nn.Linear(d_model, d_model, bias=False)\n",
    "linear_v = nn.Linear(d_model, d_model, bias=False)\n",
    "linear_q.weight.data = torch.eye(d_model)\n",
    "linear_k.weight.data = torch.eye(d_model)\n",
    "linear_v.weight.data = torch.eye(d_model)\n",
    "\n",
    "# 3. Compute Q, K, V\n",
    "Q = linear_q(X)  # Shape: (3, 4)\n",
    "K = linear_k(X)  # Shape: (3, 4)\n",
    "V = linear_v(X)  # Shape: (3, 4)\n",
    "\n",
    "# 4. Compute attention scores (Q @ K^T)\n",
    "scores = Q @ K.T  # Shape: (3, 3)\n",
    "\n",
    "# 5. Scale scores\n",
    "scaled_scores = scores / torch.sqrt(torch.tensor(d_model, dtype=torch.float32))\n",
    "\n",
    "# 6. Softmax to get attention weights\n",
    "attention_weights = F.softmax(scaled_scores, dim=-1)  # Shape: (3, 3)\n",
    "\n",
    "# 7. Compute weighted sum of V\n",
    "output = attention_weights @ V  # Shape: (3, 4)\n",
    "\n",
    "# 8. Done — print results\n",
    "print(\"Attention Weights:\\n\", attention_weights)\n",
    "print(\"Self-Attention Output:\\n\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdfce25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
