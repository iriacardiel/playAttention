{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f3cf7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "F.softargmax = F.softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03f6532a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f2661b",
   "metadata": {},
   "source": [
    "## Dense - Fully Connected (FC) Layer - nn.Linear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59b5200",
   "metadata": {},
   "source": [
    "```nn.Linear(in_features, out_features)```\n",
    "\n",
    "is equivalent to \n",
    "\n",
    "$ y = xW^T+b $ shape [1]\n",
    "\n",
    "- $ x = [3,4,5] $ shape [1x3]\n",
    "\n",
    "- $ w = [w_1, w_2, w_3] $ shape [1x3]\n",
    "\n",
    "- $ b = b $ shape [1]\n",
    "\n",
    "```in_features = 3```\n",
    "```out_features = 1```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42fdd8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = Parameter containing:\n",
      "tensor([[ 0.1873, -0.5529, -0.0038]], requires_grad=True)\n",
      "\n",
      "b = Parameter containing:\n",
      "tensor([0.4424], requires_grad=True)\n",
      "\n",
      "y = tensor([-0.4876], grad_fn=<ViewBackward0>)\n",
      "\n",
      "y_manual = tensor([-0.4876], grad_fn=<AddBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Intantiate the layer\n",
    "layer = nn.Linear(in_features=3, out_features=1, bias=True)  # weight shape = [1×3], bias = [1]\n",
    "print(f\"w = {layer.weight}\\n\")  # [1 × 3] torch tensor randomly initialized\n",
    "print(f\"b = {layer.bias}\\n\")    # [1] torch tensor randomly initialized\n",
    "\n",
    "# Use the layer and check result\n",
    "x = torch.tensor([1.0, 2.0, 3.0]) # input shape = [3]\n",
    "y = layer(x) # output shape = [1]\n",
    "#y_manual = x @ layer.weight.T + layer.bias # output shape = [1]\n",
    "y_manual = torch.matmul(x, layer.weight.T) + layer.bias # output shape = [1]\n",
    "\n",
    "assert y == y_manual\n",
    "print(f\"y = {y}\\n\")\n",
    "print(f\"y_manual = {y_manual}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbbce8b",
   "metadata": {},
   "source": [
    "## MLP (MultiLayer Perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48649133",
   "metadata": {},
   "source": [
    "**1D convolution with kernel_size = 1**\n",
    "\n",
    "This is basically an MLP with one hidden layer and ReLU activation applied to each and every element in the set.\n",
    "\n",
    "A 1D convolution with kernel_size=1 behaves like a position-wise feedforward network, or equivalently, an MLP applied independently to each position in a sequence.\n",
    "\n",
    "\n",
    "- nn.Linear applies the transformation to the last dimension of the tensor.\n",
    "\n",
    "- In transformer-style models or any sequence model, each element in the sequence can be treated independently across the sequence length when applying such layers.\n",
    "\n",
    "This models:\n",
    "\n",
    "MLP(x) = Linear(d_model → hidden_dim) → ReLU → Linear(hidden_dim → d_model)\n",
    "\n",
    "Each vector x[i] (e.g., word embedding or token representation at position i) is transformed independently.\n",
    "- Extract richer features from the input vector.\n",
    "- Introduce nonlinearity so the model can represent more complex functions.\n",
    "- Transform the input representation into a new space that's better suited for the task (e.g., classification, generation, prediction, etc.).\n",
    "- Lets you project the input to a higher-dimensional space, manipulate it there, and then bring it back. This adds depth and nonlinearity, giving the network more flexibility.\n",
    "- It's like giving the network more \"thinking room\" for each token.\n",
    "- While attention layers mix information across tokens (global context), these MLP blocks: Do not mix sequence positions. Instead, they refine each token's internal structure (its features) independently.\n",
    "\n",
    "So, self-attention = communication,\n",
    "MLP = introspection.\n",
    "\n",
    "The MLP is what gives the model depth and nonlinear transformation power at the token level. Without it, the model would be shallow and mostly linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be741262",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model,    hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, d_model)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x) # rotation\n",
    "        x = self.activation(x) # squash\n",
    "        x = self.linear2(x) # rotation\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eb4562",
   "metadata": {},
   "source": [
    "## **Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af52a75",
   "metadata": {},
   "source": [
    "### **Self-Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bbde1d",
   "metadata": {},
   "source": [
    "#### **Explanation: Self-Attention**  \n",
    "\n",
    "**Input**\n",
    "\n",
    "Let  \n",
    "$$\n",
    "x_i \\in \\mathbb{R}^n \\quad \\text{be the input token vector} \\quad\\text{for } i = 1, \\ldots, t\n",
    "$$  \n",
    "Define the input sequence matrix:  \n",
    "$$\n",
    "X \\doteq \\{x_i\\}_{i=1}^{t} \\in \\mathbb{R}^{t \\times n}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Linear Projections**  \n",
    "\n",
    "Transformation (rotation) of each token $x_i$ into query, key, and value vectors:  \n",
    "$$\n",
    "q_i = W_q x_i \\quad \\text{(Query)} \\qquad Q \\doteq \\{q_i\\}_{i=1}^{t} \\in \\mathbb{R}^{t \\times d}\n",
    "$$\n",
    "$$\n",
    "k_i = W_k x_i \\quad \\text{(Key)} \\qquad K \\doteq \\{k_i\\}_{i=1}^{t} \\in \\mathbb{R}^{t \\times d}\n",
    "$$\n",
    "$$\n",
    "v_i = W_v x_i \\quad \\text{(Value)} \\qquad V \\doteq \\{v_i\\}_{i=1}^{t} \\in \\mathbb{R}^{t \\times d}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Attention Weights**  \n",
    "\n",
    "Compute attention scores and normalize using softargmax:  \n",
    "$$\n",
    "\\text{Score i:} \\quad s_i = K^\\top q_i \\in \\mathbb{R}^{t}\n",
    "$$\n",
    "$$\n",
    "\\text{Scores:} \\quad S = QK^\\top \\in \\mathbb{R}^{t \\times t}\n",
    "$$\n",
    "$$\n",
    "\\text{Attention Weights:} \\quad A = \\text{[soft](arg)max}_\\beta(S) \\in \\mathbb{R}^{t \\times t}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Output**  \n",
    "\n",
    "Compute weighted sum of value vectors:  \n",
    "$$\n",
    "h_{i} = Va_{i} \\qquad H \\doteq \\{h_{i}\\}_{i=1}^{t} \\in \\Re^{d\\times t}\n",
    "$$\n",
    "\n",
    "$$\n",
    "H = A V\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1797f0ba",
   "metadata": {},
   "source": [
    "#### **Minimal Chronological Implementation** (Single forward pass of one layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130894c8",
   "metadata": {},
   "source": [
    "##### Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e124a8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights:\n",
      " [[0.4223188  0.1553624  0.4223188 ]\n",
      " [0.01587624 0.86681333 0.11731043]\n",
      " [0.1553624  0.4223188  0.4223188 ]]\n",
      "Self-Attention Output:\n",
      " [[0.8446376  0.73304361 0.8446376  0.73304361]\n",
      " [0.13318667 1.85093709 0.13318667 1.85093709]\n",
      " [0.5776812  1.26695639 0.5776812  1.26695639]]\n"
     ]
    }
   ],
   "source": [
    "# 1. Input: 3 tokens, each with 4 features\n",
    "X = np.array([\n",
    "    [1, 0, 1, 0],  # Token 1\n",
    "    [0, 2, 0, 2],  # Token 2\n",
    "    [1, 1, 1, 1],  # Token 3\n",
    "])  # Shape (seq_len=3, d_model=4)\n",
    "\n",
    "# 2. For reproducibility: manually initialize to identity\n",
    "W_q = np.eye(4)  # Identity for simplicity\n",
    "W_k = np.eye(4)\n",
    "W_v = np.eye(4)\n",
    "\n",
    "# 3. Compute Q, K, V\n",
    "Q = X @ W_q  # Shape (3, 4)\n",
    "K = X @ W_k  # Shape (3, 4)\n",
    "V = X @ W_v  # Shape (3, 4)\n",
    "\n",
    "# 4. Compute attention scores (Q @ K^T)\n",
    "scores = Q @ K.T  # Shape (3, 3)\n",
    "\n",
    "# 5. Scale scores\n",
    "d_k = Q.shape[1]\n",
    "scaled_scores = scores / np.sqrt(d_k)\n",
    "\n",
    "# 6. Softmax to get attention weights\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # stable softmax\n",
    "    return e_x / np.sum(e_x, axis=1, keepdims=True)\n",
    "\n",
    "attention_weights = softmax(scaled_scores)  # Shape (3, 3)\n",
    "\n",
    "# 7. Compute weighted sum of V\n",
    "output = attention_weights @ V  # Shape (3, 4)\n",
    "\n",
    "# 8. Done — print results\n",
    "print(\"Attention Weights:\\n\", attention_weights)\n",
    "print(\"Self-Attention Output:\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35e8192",
   "metadata": {},
   "source": [
    "##### Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8b868e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights:\n",
      " tensor([[0.4223, 0.1554, 0.4223],\n",
      "        [0.0159, 0.8668, 0.1173],\n",
      "        [0.1554, 0.4223, 0.4223]], grad_fn=<SoftmaxBackward0>)\n",
      "Self-Attention Output:\n",
      " tensor([[0.8446, 0.7330, 0.8446, 0.7330],\n",
      "        [0.1332, 1.8509, 0.1332, 1.8509],\n",
      "        [0.5777, 1.2670, 0.5777, 1.2670]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 1. Input: 3 tokens, each with 4 features\n",
    "X = torch.tensor([\n",
    "    [1.0, 0.0, 1.0, 0.0],  # Token 1\n",
    "    [0.0, 2.0, 0.0, 2.0],  # Token 2\n",
    "    [1.0, 1.0, 1.0, 1.0],  # Token 3\n",
    "])  # Shape: (seq_len=3, d_model=4)\n",
    "\n",
    "# 2. Linear layers for Q, K, V. For reproducibility: manually override and initialize to identity\n",
    "d_model = 4\n",
    "linear_q = nn.Linear(d_model, d_model, bias=False)\n",
    "linear_k = nn.Linear(d_model, d_model, bias=False)\n",
    "linear_v = nn.Linear(d_model, d_model, bias=False)\n",
    "linear_q.weight.data = torch.eye(d_model)\n",
    "linear_k.weight.data = torch.eye(d_model)\n",
    "linear_v.weight.data = torch.eye(d_model)\n",
    "\n",
    "# 3. Compute Q, K, V\n",
    "Q = linear_q(X)  # Shape: (3, 4)\n",
    "K = linear_k(X)  # Shape: (3, 4)\n",
    "V = linear_v(X)  # Shape: (3, 4)\n",
    "\n",
    "# 4. Compute attention scores (Q @ K^T)\n",
    "scores = Q @ K.T  # Shape: (3, 3)\n",
    "\n",
    "# 5. Scale scores\n",
    "scaled_scores = scores / torch.sqrt(torch.tensor(d_model, dtype=torch.float32))\n",
    "\n",
    "# 6. Softmax to get attention weights\n",
    "attention_weights = F.softmax(scaled_scores, dim=-1)  # Shape: (3, 3)\n",
    "\n",
    "# 7. Compute weighted sum of V\n",
    "output = attention_weights @ V  # Shape: (3, 4)\n",
    "\n",
    "# 8. Done — print results\n",
    "print(\"Attention Weights:\\n\", attention_weights)\n",
    "print(\"Self-Attention Output:\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ae0c09",
   "metadata": {},
   "source": [
    "### **Cross-Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9269c5b",
   "metadata": {},
   "source": [
    "#### **Explanation: Cross-Attention**  \n",
    "\n",
    "In self-attention, the queries (Q), keys (K), and values (V) all come from the same input $X \\doteq \\{x_i\\}_{i=1}^{t}$.\n",
    "\n",
    "In cross-attention, the queries come from one sequence (e.g., the decoder), and the keys and values come from another sequence (e.g., the encoder). This is commonly used in encoder-decoder models like transformers for machine translation.\n",
    "\n",
    "$X \\doteq \\{x_i\\}_{i=1}^{t}$ is the querys input (e.g., decoder tokens), used to compute Q\n",
    "\n",
    "$\\Xi \\doteq \\{\\xi_i\\}_{i=1}^{\\tau}$ is the key/value input (e.g., encoder outputs), used to compute K, V\n",
    "\n",
    "The length of the keys and values input is much greater than the input for the queries. This allows to search in a greater space for the context information.\n",
    "\n",
    "$$\\tau >> t$$\n",
    "\n",
    "\n",
    "**Input**\n",
    "\n",
    "Let  \n",
    "$$\n",
    "x_i \\in \\mathbb{R}^n \\quad \\text{be the input token vector (decoder tokens)} \\quad \\text{for } i = 1, \\ldots, t\n",
    "$$  \n",
    "$$\n",
    "\\xi_i \\in \\mathbb{R}^n \\quad \\text{be the key and value input token vector (encoder outputs)} \\quad \\text{for } i = 1, \\ldots, \\tau\n",
    "$$  \n",
    "\n",
    "\n",
    "Define the input sequence matrices:  \n",
    "$$\n",
    "X \\doteq \\{x_i\\}_{i=1}^{t} \\in \\mathbb{R}^{t \\times n}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Xi \\doteq \\{\\xi_i\\}_{i=1}^{\\tau} \\in \\mathbb{R}^{\\tau \\times n}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Linear Projections**  \n",
    "\n",
    "Transformation (rotation) of each token $x_i$ and $\\xi_i$ into query, key, and value vectors:  \n",
    "$$\n",
    "q_i = W_q x_i \\quad \\text{(Query)} \\qquad Q \\doteq \\{q_i\\}_{i=1}^{t} \\in \\mathbb{R}^{t \\times d}\n",
    "$$\n",
    "$$\n",
    "k_i = W_k \\xi_i \\quad \\text{(Key)} \\qquad K \\doteq \\{k_i\\}_{i=1}^{\\tau} \\in \\mathbb{R}^{\\tau \\times d}\n",
    "$$\n",
    "$$\n",
    "v_i = W_v \\xi_i \\quad \\text{(Value)} \\qquad V \\doteq \\{v_i\\}_{i=1}^{\\tau} \\in \\mathbb{R}^{\\tau \\times d}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Attention Weights**  \n",
    "\n",
    "Compute attention scores and normalize using softargmax:  \n",
    "$$\n",
    "\\text{Score i:} \\quad s_i = K^\\top q_i \\in \\mathbb{R}^{\\tau}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Scores:} \\quad S = QK^\\top \\in \\mathbb{R}^{\\tau \\times t}\n",
    "$$\n",
    "$$\n",
    "\\text{Attention Weights:} \\quad A = \\text{[soft](arg)max}_\\beta(S) \\in \\mathbb{R}^{\\tau \\times t}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Output**  \n",
    "\n",
    "Compute weighted sum of value vectors:  \n",
    "$$\n",
    "h_{i} = Va_{i} \\qquad H \\doteq \\{h_{i}\\}_{i=1}^{t} \\in \\Re^{d\\times t}\n",
    "$$\n",
    "\n",
    "$$\n",
    "H = A V\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993d2caa",
   "metadata": {},
   "source": [
    "#### **Minimal Chronological Implementation** (Single forward pass of one layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba11b472",
   "metadata": {},
   "source": [
    "#### Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b84186fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights:\n",
      " [[0.1553624  0.4223188  0.4223188 ]\n",
      " [0.66524096 0.24472847 0.09003057]]\n",
      "Cross-Attention Output:\n",
      " [[0.8446376  0.73304361 0.8446376  0.73304361]\n",
      " [0.33475904 1.57521038 0.33475904 1.57521038]]\n"
     ]
    }
   ],
   "source": [
    "# 1. Inputs: separate query input and key/value input\n",
    "X_q = np.array([\n",
    "    [1, 0, 1, 0],  # Query 1\n",
    "    [0, 1, 0, 1],  # Query 2\n",
    "])  # Shape: (2, 4)\n",
    "\n",
    "X_kv = np.array([\n",
    "    [0, 2, 0, 2],  # Key/Value 1\n",
    "    [1, 1, 1, 1],  # Key/Value 2\n",
    "    [1, 0, 1, 0],  # Key/Value 3\n",
    "])  # Shape: (3, 4)\n",
    "\n",
    "# 2. Identity weight matrices\n",
    "W_q = np.eye(4)  # For queries\n",
    "W_k = np.eye(4)  # For keys\n",
    "W_v = np.eye(4)  # For values\n",
    "\n",
    "# 3. Compute Q, K, V\n",
    "Q = X_q @ W_q      # Shape: (2, 4)\n",
    "K = X_kv @ W_k     # Shape: (3, 4)\n",
    "V = X_kv @ W_v     # Shape: (3, 4)\n",
    "\n",
    "# 4. Compute attention scores (Q @ K^T)\n",
    "scores = Q @ K.T   # Shape: (2, 3)\n",
    "\n",
    "# 5. Scale scores\n",
    "d_k = Q.shape[1]  # or K.shape[1]\n",
    "scaled_scores = scores / np.sqrt(d_k)\n",
    "\n",
    "# 6. Stable softmax\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=1, keepdims=True)\n",
    "\n",
    "attention_weights = softmax(scaled_scores)  # Shape: (2, 3)\n",
    "\n",
    "# 7. Weighted sum of values\n",
    "output = attention_weights @ V  # Shape: (2, 4)\n",
    "\n",
    "# 8. Done\n",
    "print(\"Attention Weights:\\n\", attention_weights)\n",
    "print(\"Cross-Attention Output:\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dcaaae",
   "metadata": {},
   "source": [
    "#### Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1df237be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights:\n",
      " tensor([[0.1554, 0.4223, 0.4223],\n",
      "        [0.6652, 0.2447, 0.0900]], grad_fn=<SoftmaxBackward0>)\n",
      "Cross-Attention Output:\n",
      " tensor([[0.8446, 0.7330, 0.8446, 0.7330],\n",
      "        [0.3348, 1.5752, 0.3348, 1.5752]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Two different inputs for cross-attention\n",
    "X_q = torch.tensor([\n",
    "    [1.0, 0.0, 1.0, 0.0],  # Query 1 (e.g., from decoder)\n",
    "    [0.0, 1.0, 0.0, 1.0],  # Query 2\n",
    "])  # Shape: (2, 4)\n",
    "\n",
    "X_kv = torch.tensor([\n",
    "    [0.0, 2.0, 0.0, 2.0],  # Key/Value 1 (e.g., from encoder)\n",
    "    [1.0, 1.0, 1.0, 1.0],  # Key/Value 2\n",
    "    [1.0, 0.0, 1.0, 0.0],  # Key/Value 3\n",
    "])  # Shape: (3, 4)\n",
    "\n",
    "# 2. Linear layers for Q, K, V — identity weights for simplicity\n",
    "d_model = 4\n",
    "linear_q = nn.Linear(d_model, d_model, bias=False)\n",
    "linear_k = nn.Linear(d_model, d_model, bias=False)\n",
    "linear_v = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "linear_q.weight.data = torch.eye(d_model)\n",
    "linear_k.weight.data = torch.eye(d_model)\n",
    "linear_v.weight.data = torch.eye(d_model)\n",
    "\n",
    "# 3. Compute Q from query input, K and V from key/value input\n",
    "Q = linear_q(X_q)     # Shape: (2, 4)\n",
    "K = linear_k(X_kv)    # Shape: (3, 4)\n",
    "V = linear_v(X_kv)    # Shape: (3, 4)\n",
    "\n",
    "# 4. Compute attention scores: Q @ K^T\n",
    "scores = Q @ K.T      # Shape: (2, 3)\n",
    "\n",
    "# 5. Scale\n",
    "scaled_scores = scores / torch.sqrt(torch.tensor(d_model, dtype=torch.float32))\n",
    "\n",
    "# 6. Softmax\n",
    "attention_weights = F.softmax(scaled_scores, dim=-1)  # Shape: (2, 3)\n",
    "\n",
    "# 7. Output: weighted sum of values\n",
    "output = attention_weights @ V  # Shape: (2, 4)\n",
    "\n",
    "# 8. Done\n",
    "print(\"Attention Weights:\\n\", attention_weights)\n",
    "print(\"Cross-Attention Output:\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ec6cdc",
   "metadata": {},
   "source": [
    "#### **MHA (Multi Head Attention) Module**\n",
    "\n",
    "Putting it all together with torch and batched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4771078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_input=None):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        if d_input is None:\n",
    "            d_xq = d_xk = d_xv = d_model\n",
    "        else:\n",
    "            d_xq, d_xk, d_xv = d_input\n",
    "            \n",
    "        # Make sure that the embedding dimension of model is a multiple of number of heads\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.d_k = d_model // self.num_heads\n",
    "        \n",
    "        # These are still of dimension d_model. They will be split into number of heads \n",
    "        self.W_q = nn.Linear(d_xq, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_xk, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_xv, d_model, bias=False)\n",
    "        \n",
    "        # Outputs of all sub-layers need to be of dimension d_model\n",
    "        self.W_h = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V):\n",
    "        batch_size = Q.size(0) \n",
    "        k_length = K.size(-2) \n",
    "        \n",
    "        # Scaling by d_k so that the softargmax doesnt saturate\n",
    "        Q = Q / np.sqrt(self.d_k)                   # (bs, n_heads, q_length, dim_per_head)\n",
    "        scores = torch.matmul(Q, K.transpose(2,3))  # (bs, n_heads, q_length, k_length)\n",
    "        \n",
    "        A = F.softargmax(scores, dim=-1)            # (bs, n_heads, q_length, k_length)\n",
    "        \n",
    "        # Get the weighted average of the values\n",
    "        H = torch.matmul(A, V)                      # (bs, n_heads, q_length, dim_per_head)\n",
    "\n",
    "        return H, A \n",
    "\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        Split the last dimension into (heads X depth)\n",
    "        Return after transpose to put in shape (batch_size X num_heads X seq_length X d_k)\n",
    "        \"\"\"\n",
    "        return x.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "    def group_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        Combine the heads again to get (batch_size X seq_length X (num_heads times d_k))\n",
    "        \"\"\"\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
    "    \n",
    "\n",
    "    def forward(self, X_q, X_k, X_v):\n",
    "        batch_size, seq_length, dim = X_q.size()\n",
    "\n",
    "        # After transforming, split into num_heads \n",
    "        Q = self.split_heads(self.W_q(X_q), batch_size)  # (bs, n_heads, q_length, dim_per_head)\n",
    "        K = self.split_heads(self.W_k(X_k), batch_size)  # (bs, n_heads, k_length, dim_per_head)\n",
    "        V = self.split_heads(self.W_v(X_v), batch_size)  # (bs, n_heads, v_length, dim_per_head)\n",
    "        \n",
    "        # Calculate the attention weights for each of the heads\n",
    "        H_cat, A = self.scaled_dot_product_attention(Q, K, V)\n",
    "        \n",
    "        # Put all the heads back together by concat\n",
    "        H_cat = self.group_heads(H_cat, batch_size)    # (bs, q_length, dim)\n",
    "        \n",
    "        # Final linear layer  \n",
    "        H = self.W_h(H_cat)          # (bs, q_length, dim)\n",
    "        \n",
    "        return H, A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abab661a",
   "metadata": {},
   "source": [
    "##### Sanity checks for the MHA\n",
    "\n",
    "To check our self attention works - if the query matches with one of the key values, it should have all the attention focused there, with the value returned as the value at that index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f72e3d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_MHA(Q, K, V):\n",
    "    H, A = MultiHeadAttention(d_model=512, num_heads=8).scaled_dot_product_attention(Q, K, V)\n",
    "    print('Attention weights A =', A.squeeze())\n",
    "    print('Output H =', H.squeeze())\n",
    "    \n",
    "    \n",
    "test_K = torch.tensor(\n",
    "    [[10, 0, 0], # strong in \"x\" direction\n",
    "     [ 0,10, 0], # strong in \"y\" direction ✅\n",
    "     [ 0, 0,10], # strong in \"z\" direction\n",
    "     [ 0, 0,10]] # strong in \"z\" direction\n",
    ").float()[None,None]\n",
    "\n",
    "test_V = torch.tensor(\n",
    "    [[   1,0,0],\n",
    "     [  10,0,0], # ✅\n",
    "     [ 100,5,0],\n",
    "     [1000,6,0]]\n",
    ").float()[None,None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9df7651d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights A = tensor([3.7266e-06, 9.9999e-01, 3.7266e-06, 3.7266e-06])\n",
      "Output H = tensor([1.0004e+01, 4.0993e-05, 0.0000e+00])\n"
     ]
    }
   ],
   "source": [
    "test_Q = torch.tensor(\n",
    "    [[0, 10, 0]] # strong in \"y\" direction\n",
    ").float()[None,None]\n",
    "\n",
    "testing_MHA(test_Q, test_K, test_V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5520cc92",
   "metadata": {},
   "source": [
    "The query [0,10,0] matches the second key [0,10,0] and so the second value is returned [10,0,0]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5df7d15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights A = tensor([1.8633e-06, 1.8633e-06, 5.0000e-01, 5.0000e-01])\n",
      "Output H = tensor([549.9979,   5.5000,   0.0000])\n"
     ]
    }
   ],
   "source": [
    "test_Q = torch.tensor(\n",
    "    [[0, 0, 10]]\n",
    "    ).float()  \n",
    "testing_MHA(test_Q, test_K, test_V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b36b64",
   "metadata": {},
   "source": [
    "If we give a query that matches two keys exactly, it should return the averaged value of the two values for those two keys. We see that it focuses equally on the third and fourth key and returns the average of their values.\n",
    "\n",
    "Now giving all the queries at the same time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f9bd473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights A = tensor([[1.8633e-06, 1.8633e-06, 5.0000e-01, 5.0000e-01],\n",
      "        [3.7266e-06, 9.9999e-01, 3.7266e-06, 3.7266e-06],\n",
      "        [5.0000e-01, 5.0000e-01, 1.8633e-06, 1.8633e-06]])\n",
      "Output H = tensor([[5.5000e+02, 5.5000e+00, 0.0000e+00],\n",
      "        [1.0004e+01, 4.0993e-05, 0.0000e+00],\n",
      "        [5.5020e+00, 2.0497e-05, 0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_Q = torch.tensor(\n",
    "    [[0, 0, 10], [0, 10, 0], [10, 10, 0]]\n",
    ").float()[None,None]\n",
    "testing_MHA(test_Q, test_K, test_V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecac17c8",
   "metadata": {},
   "source": [
    "## Transformer encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923f8044",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "Self attention by itself does not have any recurrence or convolutions so to make it sensitive to position we must provide additional positional encodings. These are calculated as follows:\n",
    "\n",
    "$$\n",
    "E(p,2i) = sin(p/10000^{2i/d})\n",
    "$$\n",
    "\n",
    "$$\n",
    "E(p,2i+1) = cos(p/10000^{2i/d})\n",
    "$$\n",
    "\n",
    "The embeddings are the sum of positional embedding + token embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c23380c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sinusoidal_embeddings(nb_p, dim, E):\n",
    "    theta = np.array([\n",
    "        [p / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)]\n",
    "        for p in range(nb_p)\n",
    "    ])\n",
    "    E[:, 0::2] = torch.FloatTensor(np.sin(theta[:, 0::2]))\n",
    "    E[:, 1::2] = torch.FloatTensor(np.cos(theta[:, 1::2]))\n",
    "    E.requires_grad = False\n",
    "    E = E.to(device)\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size, max_position_embeddings):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, d_model, padding_idx=1)\n",
    "        self.position_embeddings = nn.Embedding(max_position_embeddings, d_model)\n",
    "        create_sinusoidal_embeddings(\n",
    "            nb_p=max_position_embeddings,\n",
    "            dim=d_model,\n",
    "            E=self.position_embeddings.weight\n",
    "        )\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(d_model, eps=1e-12)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device) # (max_seq_length)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)                      # (bs, max_seq_length)\n",
    "        \n",
    "        # Get word embeddings for each input id\n",
    "        word_embeddings = self.word_embeddings(input_ids)                   # (bs, max_seq_length, dim)\n",
    "        \n",
    "        # Get position embeddings for each position id \n",
    "        position_embeddings = self.position_embeddings(position_ids)        # (bs, max_seq_length, dim)\n",
    "        \n",
    "        # Add them both \n",
    "        embeddings = word_embeddings + position_embeddings  # (bs, max_seq_length, dim)\n",
    "        \n",
    "        # Layer norm \n",
    "        embeddings = self.LayerNorm(embeddings)             # (bs, max_seq_length, dim)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca27731d",
   "metadata": {},
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e733720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, conv_hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = FeedForward(d_model, conv_hidden_dim)\n",
    "        self.layernorm1 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "        self.layernorm2 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Multi-head attention (self-attention)\n",
    "        attn_output, _ = self.mha(x, x, x)  # (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        # Layer norm after adding the residual connection \n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        # Feed forward \n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        #Second layer norm after adding residual connection \n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c17e3a",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "Input embedding (token + positional) + Blocks of N Encoder Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "656fb25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, ff_hidden_dim, input_vocab_size,\n",
    "               maximum_position_encoding):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = Embeddings(d_model, input_vocab_size,maximum_position_encoding)\n",
    "\n",
    "        self.enc_layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.enc_layers.append(EncoderLayer(d_model, num_heads, ff_hidden_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) # Transform to (batch_size, input_seq_length, d_model)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
